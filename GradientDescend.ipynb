{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69da7c29-537f-45e1-a5a5-5983f4a08e0e",
   "metadata": {},
   "source": [
    "                                                \n",
    " #  Gradient Descent notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook examines the mechanism of gradient descent and its dependence on key parameter like the number of iterations and the learning rate. The goal is to understand how gradient descent works and to gain hands-on experience with TensorFlow.\n",
    "\n",
    "I begin with a simple case: a one-dimensional function. An interactive widget lets users adjust parameters and visually observe the gradient descent path. This provides an intuitive understanding of how the algorithm converges.\n",
    "\n",
    "Next, I extend the exploration to two dimensions using TensorFlow. The gradient descent path is plotted on a three-dimensional surface representing the function. The widget shows how the optimization process works on more than one dimension. \n",
    "\n",
    "Finally, I present another interactive widget to visualize the gradient descent path on the two-dimensional level curves of the function, offering a complementary perspective on the optimization process.\n",
    "\n",
    "Through these exercises, the notebook aims to strengthen understanding of both the gradient descent algorithm and the fundamentals of TensorFlow. \n",
    "\n",
    "Part of  code is an adaptation of the code presented in file Utah_Gradient (see references)\n",
    "                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c46e60f-c90b-4fb9-ba6a-5d730cccc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "# to be used for two  the dimensional gradient\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f743f-84bc-4cfc-b750-ee6fe2d7a9e2",
   "metadata": {},
   "source": [
    "## 1)  Gradient descent in one dimension \n",
    "\n",
    "Consider the simple cuadratic funtion\n",
    "$$\n",
    "f(x) = (x - 2)^2\\,.\n",
    "$$\n",
    "The gradient descent method  is an iterative process that finds the point $x^*$ that minimize the function or the best point near to the minimum. \n",
    "To reach this point starting from a point $x^i$, one  moves in the direction of decreasing function values. This direction is given by the opposite sign of the derivative $f^\\prime$.  The movement is done by small steps $\\epsilon$ iteratively and is described by the equation\n",
    "\n",
    "$$\n",
    "x^{i+1}  = x^i - \\epsilon   \\, f^\\prime (x^i) \\,,\n",
    "$$\n",
    "where   $\\epsilon$ is a positive scalar called the **learning rate**. The process stops until certain criteria are accomplished. \n",
    "In these notes, the algorithm stops when the maximum number of iterations is reached. It works becase I am considering a  convex quadratic function \n",
    "which has only a global minimum. For more complex functions with several local minima or with stationary points other stopping criteria have to be considered. \n",
    "\n",
    "There are several proposals to choose  the step $\\epsilon$. Here, $\\epsilon$ will be considered constant throughout all iterations. \n",
    "\n",
    "The following shows a widget with number of iterations $n= 15$ and different values of learning rates (from 0.01 to 1). \n",
    "For each value of learning rate, one can iterate along the 15 iterations. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7615812d-f4c8-4515-b5e2-17db9459ae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76d76cdecf8476aaa41e4ae5fd55ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='Iterations', max=15, min=1), FloatSlider(value=0.5, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Example quadratic function\n",
    "# ==========================================\n",
    "# define the function\n",
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "# define the derivatie \n",
    "def gradient(x):\n",
    "    return  2*(x-2)\n",
    "\n",
    "# define the algorithm\n",
    "def gradient_descent(learning_rate, n_iterations):\n",
    "    x = 5.0  # initial point\n",
    "    positions = [x]\n",
    "    for _ in range(n_iterations):\n",
    "        x = x - learning_rate * gradient(x)\n",
    "         # stop if x explodes\n",
    "        if abs(x) > 1e6:\n",
    "            break\n",
    "        positions.append(x)\n",
    "    return positions\n",
    "\n",
    "# define the plot and variables x and y for the plot\n",
    "def plot_gd(n_iterations, learning_rate):\n",
    "    xdata = np.linspace(-1, 6, 200) \n",
    "    ydata = f(xdata)\n",
    "\n",
    "    # Run Gradient Descent\n",
    "    x_val = gradient_descent(learning_rate, n_iterations)\n",
    "    y_val = [f(x) for x in x_val]\n",
    "\n",
    "    # Plot the function\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(xdata, ydata, label=\"f(x) = (x-2)²\")\n",
    "    plt.scatter(x_val, y_val, c=\"red\")\n",
    "    plt.plot(x_val, y_val, c=\"red\", linestyle=\"--\", label=\"Gradient descent path\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.title(r\"Gradient Descent with 15 iterations and several learning rates\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive sliders\n",
    "interact(plot_gd, \n",
    "         n_iterations=IntSlider(value=5, min=1, max=15, step=1, description=\"Iterations\"),\n",
    "         learning_rate=FloatSlider(value=0.5, min=0.01, max=1.0, step=0.01, description=\"Learning Rate\"));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5fb6f-718a-42d2-bca6-f1404019222e",
   "metadata": {},
   "source": [
    "* From the widget above, it is possible to see that for  $\\epsilon < 0.12$, the minimum of the functions is not reached after the 15 interations.\n",
    "* For a learning rate equal to $0.5$, the minimum is reached in just one step (one iteration)\n",
    "* When $\\epsilon = 0.2$, the minimum is reached smoothly among the 15 iterations. It seems to be an optimal value for the learning rate.\n",
    "* On the other hand, for  learning values above $0.57$, the algorithm starts to oscillate around the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a3aad-d36f-468f-ac75-6b9f3420fab7",
   "metadata": {},
   "source": [
    "##  2) Gradient Descent of a two-dimensional function\n",
    "\n",
    "In this case, for a function of two variables $f(x,y)$ one has to consider\n",
    "\n",
    "$$\n",
    "\\vec{v}^{i+1} = \\vec{v}^i + \\epsilon \\, \\vec{\\nabla} f(\\vec{v}^i)\n",
    "$$\n",
    "where $\\vec{v} = (x,y)$ and $\\vec{\\nabla}f$ is the gradient of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545586cf-21f5-4e2f-be71-9cb3298d35f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5005c98528740c8af678ce86cd99c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, description='learning_rate', max=1.0, min=0.01, step=0.01), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Gradient Descent with TensorFlow\n",
    "# ====================================================\n",
    "   \n",
    "#  Ordered triples (x, y, z) for plotting\n",
    "def function(vec):\n",
    "    \"\"\" \n",
    "    Argument: \n",
    "    vec         -- vector v = (x,y) \n",
    "    \n",
    "    Return:\n",
    "    space_point --  a 3D point represented as the vector (x,y,z)\n",
    "    \"\"\"\n",
    "    x, y = vec[0], vec[1]\n",
    "    z =  (0.75 * x - 1.5) ** 2 + (y - 2.0) ** 2 + 0.25 * x * y\n",
    "    space_point = tf.stack([x, y, z], axis=0)\n",
    "    return space_point\n",
    "\n",
    "# Gradient vector\n",
    "def gradient(vec):\n",
    "    \"\"\" \n",
    "    Argument: \n",
    "    vec         -- vector v = (x,y) \n",
    "    \n",
    "    Return:\n",
    "    grad_fun --  2D gradient vector (df/dx, df/dy)\n",
    "    \"\"\"\n",
    "    x, y = vec[0], vec[1]\n",
    "    df_dx = 1.125 * x - 2.25 + 0.25 * y\n",
    "    df_dy = 2.0 * y - 4.0 + 0.25 * x\n",
    "    grad_fun = tf.stack([df_dx, df_dy], axis=0)\n",
    "    return grad_fun\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(learning_rate, num_iter):\n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    learning_rate -- the learning_rate \n",
    "    num_iter      -- number of iterarions\n",
    "\n",
    "    Return:\n",
    "    \n",
    "    \"\"\"\n",
    "    v_init = tf.constant([5.0, 4.0], dtype=tf.float32)\n",
    "    v = v_init\n",
    "\n",
    "    update_v = [v_init]\n",
    "    grad_norms = []\n",
    "    #points = []       # store function evaluations\n",
    "    \n",
    "    for i in range(1, num_iter + 1):\n",
    "        grad = gradient(v)\n",
    "        v = v - learning_rate *  grad\n",
    "        update_v.append(v)\n",
    "        grad_norms.append(tf.norm(grad).numpy())\n",
    "       \n",
    "\n",
    "    # store the update values for the plot\n",
    "    points = []\n",
    "    for k in range(len(update_v)):\n",
    "        point= function(update_v[k])\n",
    "        points.append(point)\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting them\n",
    "    points_np = [p.numpy() for p in points]\n",
    "\n",
    "    # Split into x, y, z lists\n",
    "    x_vals = [p[0] for p in points_np]\n",
    "    y_vals = [p[1] for p in points_np]\n",
    "    z_vals = [p[2] for p in points_np]\n",
    "    \n",
    "\n",
    "    # Create a grid for contour plotting and plotting the function\n",
    "    xdata = tf.linspace(-1.0, 6.0, 50)\n",
    "    ydata = tf.linspace(-1.0, 6.0, 50)\n",
    "    X, Y = tf.meshgrid(xdata, ydata)\n",
    "    # Define the functions of two variables\n",
    "    Z =  (0.75*X -1.5)**2 + (Y-2.0)**2 + 0.25*X*Y\n",
    "    # Plot the surface\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, edgecolor='none', alpha=0.6 )\n",
    "    \n",
    "    # Define the levels for the contour plot\n",
    "    lev = tf.linspace(0, 20, 21)\n",
    "    # Plot the 3D contour lines\n",
    "    ax.contour3D(X, Y, Z, levels=lev, cmap='gray')\n",
    "\n",
    "    #  Plot gradient descent path\n",
    "    ax.scatter(x_vals, y_vals, z_vals, color=\"blue\", s=30, label=\"Points\")\n",
    "    # Optionally connect them with a line\n",
    "    ax.plot(x_vals, y_vals, z_vals, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # Set plot labels\n",
    "    ax.set_xlabel('x axis')\n",
    "    ax.set_ylabel('y axis')\n",
    "    ax.set_zlabel('z axis')\n",
    "    ax.set_title('z = f(x,y)')\n",
    "    plt.title(f'Gradient Descent Path on the surface of the 3D plot ')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    final_point =  update_v[-1]\n",
    "    final_grad = grad_norms[-1]\n",
    "    print(\"Final point:\", final_point.numpy())\n",
    "    print(\"Final Gradient norm:\",final_grad)\n",
    "\n",
    "# Create interactive widget\n",
    "interact(gradient_descent,\n",
    "         learning_rate=FloatSlider(value=0.9, min=0.01, max=1.0, step=0.01),\n",
    "         num_iter=IntSlider(value=10, min=1, max=15));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1789e7-15f6-4b49-8dbf-601bc4200888",
   "metadata": {},
   "source": [
    "## 3) Gradient Descent of 2D  plots\n",
    "The path of the descent gradient can be also visualized on the 2D plot of the level curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f617688d-1da7-468f-b4b6-6f4fbe68850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912426103d14d53a56b970b3a77dcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, description='learning_rate', max=1.0, min=0.01, step=0.01), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Visualization of the Gradient Descent Path on the plane\n",
    "# ====================================================\n",
    "\n",
    "# Define the function and gradient\n",
    "def function(vec):\n",
    "    x, y = vec[0], vec[1]\n",
    "    z = (0.75 * x - 1.5) ** 2 + (y - 2.0) ** 2 + 0.25 * x * y\n",
    "    return tf.stack([x, y, z], axis=0)\n",
    "\n",
    "def gradient(vec):\n",
    "    x, y = vec[0], vec[1]\n",
    "    df_dx = 1.125 * x - 2.25 + 0.25 * y\n",
    "    df_dy = 2.0 * y - 4.0 + 0.25 * x\n",
    "    return tf.stack([df_dx, df_dy], axis=0)\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(learning_rate, num_iter):\n",
    "    v_init = tf.constant([5.0, 4.0], dtype=tf.float32)\n",
    "    v = v_init\n",
    "\n",
    "    update_vals = [v_init]\n",
    "    grad_norms = []\n",
    "\n",
    "    for i in range(1, num_iter + 1):\n",
    "        grad = gradient(v)\n",
    "        v = v - learning_rate * grad\n",
    "        update_vals.append(v)\n",
    "        grad_norms.append(tf.norm(grad).numpy())\n",
    "\n",
    "    # Create grid for contour plot\n",
    "    xdata = tf.linspace(-1.0, 6.0, 50)\n",
    "    ydata = tf.linspace(-1.0, 6.0, 50)\n",
    "    X, Y = tf.meshgrid(xdata, ydata)\n",
    "    Z = (0.75*X - 1.5)**2 + (Y - 2.0)**2 + 0.25*X*Y\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contour(X, Y, Z, levels=20)\n",
    "\n",
    "    # Plot gradient descent path\n",
    "    vals = tf.stack(update_vals)\n",
    "    plt.plot(vals[:, 0], vals[:, 1], marker='o', color='blue')\n",
    "    # Optionally connect them with a line\n",
    "    plt.plot(vals[:, 0], vals[:, 1], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f'Gradient Descent Path on the 2D level curves')\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final point:\", update_vals[-1].numpy())\n",
    "    print(\"Gradient norms:\", grad_norms[-1])\n",
    "\n",
    "# Create interactive widget\n",
    "interact(gradient_descent,\n",
    "         learning_rate=FloatSlider(value=0.9, min=0.01, max=1.0, step=0.01),\n",
    "         num_iter=IntSlider(value=10, min=1, max=50));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19471f1-7eeb-4ab2-b599-e1f7c58d6e79",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "* [Utah_Gradient](https://users.cs.utah.edu/~jeffp/IDABook/T6-GD.pdf)\n",
    "* [Medium_Tensor_Flow](https://igormintz.medium.com/basic-tensor-creation-and-manipulation-with-tensorflow-ee4c910a00e2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
